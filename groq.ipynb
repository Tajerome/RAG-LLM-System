{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taj\\Documents\\Procurement_chatbot\\ollama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from uuid import uuid4 \n",
    "from groq import Groq\n",
    "import gradio as gr\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4 \n",
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "load_dotenv()\n",
    "\n",
    "# Load local embedding model (768-dim)\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety and Relevance Guardrail Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: safety-only check ---\n",
    "import re\n",
    "\n",
    "def is_safe_and_relevant(query):\n",
    "    \"\"\"\n",
    "    Safety-only check. Returns (True, \"\") if safe.\n",
    "    This function DOES NOT decide relevance. Relevance is decided\n",
    "    later by the main LLM (llama-3.3-70b-versatile).\n",
    "    \"\"\"\n",
    "    if not isinstance(query, str) or not query.strip():\n",
    "        return False, \"Empty or invalid query.\"\n",
    "\n",
    "    lowered = query.lower()\n",
    "\n",
    "    # Simple unsafe patterns (customize as needed)\n",
    "    unsafe_patterns = [\n",
    "        r\"\\bkill\\b\", r\"\\bterror\\b\", r\"\\bself[- ]?harm\\b\", r\"\\bsuicid\\b\",\n",
    "        r\"\\bsexual\\b\", r\"\\bsex\\b\", r\"\\bchild\\b\", r\"\\bexplosive\\b\",\n",
    "        r\"\\bbomb\\b\", r\"\\bpoison\\b\", r\"\\bhack\\b\", r\"\\bpassword\\b\",\n",
    "        r\"\\bssn\\b\", r\"\\bcredit card\\b\", r\"\\bshoot\\b\", r\"\\battack\\b\",\n",
    "        r\"\\bweapon\\b\", r\"\\bdrugs?\\b\"\n",
    "    ]\n",
    "\n",
    "    for pat in unsafe_patterns:\n",
    "        if re.search(pat, lowered):\n",
    "            return False, \"Your question contains unsafe content and cannot be processed.\"\n",
    "\n",
    "    return True, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_guard(question: str, retriever, llama_guard):\n",
    "    # --- Guard the input ---\n",
    "    if not llama_guard(question):\n",
    "        return \" Question blocked by Llama Guard.\"\n",
    "\n",
    "    # --- Get context from retriever ---\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if not docs:\n",
    "        return \"I don't know based on the provided documents.\"\n",
    "\n",
    "    #  Keep only your PDF sources, drop web ones\n",
    "    filtered_docs = []\n",
    "    for d in docs:\n",
    "        meta = getattr(d, \"metadata\", {})\n",
    "        src = str(meta.get(\"source\", \"\")).lower()\n",
    "        if src.endswith(\".pdf\"):   # only your uploaded syllabus/linear algebra\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    if not filtered_docs:\n",
    "        return \"I don't know based on the provided documents.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{i+1}] {d.page_content}\" for i, d in enumerate(filtered_docs)\n",
    "    )\n",
    "\n",
    "    # --- Strict system prompt ---\n",
    "    system_prompt = (\n",
    "        \"You are a STRICT retrieval-augmented assistant.\\n\"\n",
    "        \"- Only answer from the provided context.\\n\"\n",
    "        \"- If the context is insufficient, reply exactly:\\n\"\n",
    "        \"\\\"I don't know based on the provided documents.\\\"\\n\"\n",
    "        \"- Always cite sources like [1], [2].\\n\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "\n",
    "    # --- Call Groq LLM ---\n",
    "    draft = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=700,\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    # --- Guard the output ---\n",
    "    if not llama_guard(draft):\n",
    "        return \"❌ Answer blocked by Llama Guard.\"\n",
    "\n",
    "    return draft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internet Search Utility\n",
    "import os\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_SEARCH_ENGINE_ID = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\n",
    "\n",
    "def google_search(query, num_results=5):\n",
    "    \"\"\"Perform a Google Custom Search and return text snippets.\"\"\"\n",
    "    print(\"Searching the internet...\")  # Required for visibility\n",
    "\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": GOOGLE_SEARCH_ENGINE_ID,\n",
    "        \"q\": query,\n",
    "        \"num\": num_results,\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    results = []\n",
    "    for item in data.get(\"items\", []):\n",
    "        snippet = item.get(\"snippet\", \"\")\n",
    "        link = item.get(\"link\", \"\")\n",
    "        results.append(f\"{snippet} (Source: {link})\")\n",
    "\n",
    "    return \"\\n\".join(results) if results else \"No relevant web results found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Web Search + Per-Source Scoring + Rendering\n",
    "# This leaves your original `google_search` untouched.\n",
    "# Dependencies: GOOGLE_API_KEY / GOOGLE_SEARCH_ENGINE_ID env, `requests` imported earlier, `embedding_model` available.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def google_search_structured(query: str, num_results: int = 10):\n",
    "    \"\"\"\n",
    "    Google CSE — returns a list of dicts with title, link, snippet.\n",
    "    Prints 'Searching the internet...' (as required).\n",
    "    \"\"\"\n",
    "    print(\"Searching the internet...\")\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        \"cx\": os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\"),\n",
    "        \"q\": query,\n",
    "        \"num\": num_results,\n",
    "    }\n",
    "    resp = requests.get(url, params=params)\n",
    "    data = resp.json()\n",
    "    results = []\n",
    "    for item in data.get(\"items\", []):\n",
    "        results.append({\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"link\": item.get(\"link\", \"\"),\n",
    "            \"snippet\": item.get(\"snippet\", \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def render_sources_with_scores(results: list, header: str = \"Web Sources\"):\n",
    "    \"\"\"\n",
    "    Pretty-prints sources with individual scores (Title, URL, Snippet, Score).\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return f\"=== {header} ===\\nNo web sources found.\"\n",
    "    lines = [f\"=== {header} (Top {len(results)}) ===\"]\n",
    "    for i, r in enumerate(results, 1):\n",
    "        lines.append(\n",
    "            f\"[{i}] {r.get('title','')}\\n\"\n",
    "            f\"URL: {r.get('link','')}\\n\"\n",
    "            f\"Score: {r.get('score','0')}\\n\"\n",
    "            f\"Snippet: {r.get('snippet','')}\\n\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq API Connection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you didn't ask a question or provide any text for me to respond to. Could you please provide more context or ask a question so I can assist you better? I'm here to help with any information or topics you're interested in.\n"
     ]
    }
   ],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "question = input(\"What is your Question?\")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Vector Database Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client (connecting to existing index)\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index('procurement-chatbot')\n",
    "\n",
    "# Load embedding model for queries only  \n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text=\"None\"):\n",
    "    embedding = embedding_model.encode(text).tolist()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prototyping the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LLM Generated Response ---\n",
      "It appears you haven't provided a question for me to answer based on the given context. Please provide the question, and I'll be happy to assist you. If the answer is not available in the context, I will state that I cannot answer from the provided information.\n",
      "\n",
      "--- Retrieved Relevant Chunks with Metadata ---\n",
      "\n",
      "Chunk 1:\n",
      "  Score: 0.2451\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who’s Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "  Score: 0.1941\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 23.0\n",
      "  Content:\n",
      "Sequence Learning & GANs Week 19\n",
      "Learning Obje ctives: This week, we explore another fascinating application of neural networks: equipping \n",
      "computers to understand human language. You will learn to work with text data and sequential data, delving into the \n",
      "world of Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs).\n",
      "RNNs are designed to recognize patterns in sequences of data, making them ideal for tasks involving time series,\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "  Score: 0.1898\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 25.0\n",
      "  Content:\n",
      "WWW.iNNOVATiCS.AiDATA SCIENCE & AI ZERO TO HERO\n",
      "Advanced Natural Language Processing (NLP) Week 21\n",
      "Learning Objectives: This week, we dive into advanced Natural Language Processing (NLP) modeling. As NLP \n",
      "continues to evolve, it is transforming the way we interact with machines and manage information. Advanced NLP \n",
      "techniques, such as deep learning-based models, are pushing the boundaries of what machines can understand and\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 4:\n",
      "  Score: 0.1835\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 8.0\n",
      "  Content:\n",
      "complete overhaul of the company’s online presence, enhancing digital sales and marketing performance with a focus on data science, AI, cloud computing, and \n",
      "cybersecurity training.In addition to leading Digirabia. He is dedicated to helping businesses increase their customer base, sales, and revenue through strategic \n",
      "consulting and coaching, covering areas such as digital marketing, ad tech, mar tech, and data-driven AI marketing.\n",
      "Pioneering Leader and Innovator in Digital Transformation\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 5:\n",
      "  Score: 0.1829\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "www.innovatics.ai\n",
      "+1 (678) 209 9780\n",
      "register@innovatics.ai\n",
      "From Zero to Hero 40-Week Training Program\n",
      " LIVE ONLINE \n",
      "TRAINING PROGRAM\n",
      "BATCH 22\n",
      "FEBruary\n",
      "202522\n",
      "nd\n",
      "40 WEEKS\n",
      "The CPD Certification\n",
      "Service\n",
      "Marquis Who’s Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Get the user query\n",
    "user_query = input(\"Ask something: \")\n",
    "\n",
    "# Convert the query to an embedding\n",
    "query_embedding = embedding_model.encode(user_query).tolist()\n",
    "\n",
    "# Search Pinecone index\n",
    "top_k = 5\n",
    "results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "# Extract relevant chunks for LLM context\n",
    "relevant_chunks_text = [match['metadata']['text'] for match in results['matches']]\n",
    "\n",
    "# Combine chunks into a single context string\n",
    "context = \"\\n\\n\".join(relevant_chunks_text)\n",
    "\n",
    "# Formulate the prompt for the LLM\n",
    "prompt_for_llm = f\"\"\"Based on the following context, please answer the question.\n",
    "If the answer is not available in the context, state that you cannot answer from the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Call the Groq LLM for inference\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_for_llm,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\", # Or preferred Groq model\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# Print the LLM's generated response\n",
    "print(\"\\n--- LLM Generated Response ---\")\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n",
    "print(\"\\n--- Retrieved Relevant Chunks with Metadata ---\")\n",
    "for i, match in enumerate(results['matches']):\n",
    "    chunk_text = match['metadata']['text']\n",
    "    filename = match['metadata'].get('filename', 'N/A')\n",
    "    page_number = match['metadata'].get('page_number', 'N/A')\n",
    "    score = match['score']\n",
    "\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Score: {score:.4f}\")\n",
    "    print(f\"  Source File: {filename}\")\n",
    "    print(f\"  Page Number: {page_number}\")\n",
    "    print(f\"  Content:\\n{chunk_text}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "#  Smart RAG Query Handler with Agentic Web Search\n",
    "# ================================\n",
    "\n",
    "# System prompt for LLM\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that answers using the following sources in order:\n",
    "\n",
    "1. **Primary Source:** The provided document context (RAG retrieval results)\n",
    "2. **Secondary Source:** If available, additional web search results\n",
    "\n",
    "Instructions:\n",
    "- Prefer document context first. If the answer is clearly not in the documents, \n",
    "it is likely off-topic..\n",
    "-Only answer questions based off of provided documents. Must be relevant to the provided documents \n",
    "- Only use the web if documents don't answer the query.\n",
    "- If both sources are insufficient, respond:\n",
    "\"I'm sorry, I couldn't find enough relevant information to answer that.\"\n",
    "- Follow these behavioral guidelines:\n",
    "    - If unsure, admit it instead of guessing or hallucinating.\n",
    "    - Only say a question is unrelated if it clearly has no connection to provided documents\n",
    "    - Avoid generic or evasive responses. Be specific and clear, even if the answer is limited.\n",
    "    - Do not reject questions that are vague or short; instead, ask clarifying questions if needed.\n",
    "    - Assume the user may be continuing a conversation and handle follow-ups like \"elaborate\" or \"explain more.\"\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def smart_rag_query(user_query):\n",
    "    \"\"\"\n",
    "    1. Retrieves context from RAG.\n",
    "    2. LLM decides if web search is needed.\n",
    "    3. Combines web results with RAG context if needed.\n",
    "    4. Returns final LLM answer using system prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Safety & relevance check\n",
    "    is_safe, msg = is_safe_and_relevant(user_query)\n",
    "    if not is_safe:\n",
    "        return msg\n",
    "\n",
    "    # --- Step 2: Retrieve context from your existing RAG pipeline\n",
    "    # ⚠ Replace with your actual retrieval function if named differently\n",
    "    rag_context = retrieve_context(user_query)\n",
    "    context_text = \"\\n\".join(rag_context) if isinstance(rag_context, list) else rag_context\n",
    "\n",
    "    # --- Step 3: Ask LLM if web search is needed\n",
    "    decision_prompt = f\"\"\"\n",
    "    You are a decision-making agent.\n",
    "    User Query: {user_query}\n",
    "    RAG Context (truncated): {context_text[:1000]}\n",
    "    \n",
    "    Decide if the retrieved document context is sufficient to answer.\n",
    "    Reply with exactly one word: \"SEARCH\" or \"NO SEARCH\"\n",
    "    \"\"\".strip()\n",
    "\n",
    "    decision = llm_generate_response(decision_prompt).strip().upper()\n",
    "    print(f\"[DEBUG] LLM Decision: {decision}\")  # For testing\n",
    "\n",
    "    # --- Step 4: Perform web search if LLM decides so\n",
    "    if \"SEARCH\" in decision:\n",
    "        web_results = google_search(user_query)\n",
    "        combined_context = f\"{context_text}\\n\\nAdditional Web Info:\\n{web_results}\"\n",
    "    else:\n",
    "        combined_context = context_text\n",
    "\n",
    "    # --- Step 5: Generate final response using LLM with system prompt\n",
    "    final_prompt = f\"\"\"\n",
    "    {system_prompt}\n",
    "\n",
    "    Document & Web Context:\n",
    "    {combined_context}\n",
    "\n",
    "    User Query: {user_query}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    final_answer = llm_generate_response(final_prompt)\n",
    "    return final_answer.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Function with Integrated Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_with_guardrails(query):\n",
    "    \"\"\"\n",
    "    Retrieve top-k docs, show scores + previews, then ask\n",
    "    llama-3.3-70b-versatile to DECIDE relevance and answer only if supported.\n",
    "    NO web fallback. If out-of-scope, it refuses.\n",
    "    \"\"\"\n",
    "    # 1) Embed query\n",
    "    query_embedding = embedding_model.encode([query])[0].tolist()\n",
    "\n",
    "    # 2) Query Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=15, include_metadata=True)\n",
    "    matches = results.get(\"matches\", []) if isinstance(results, dict) else getattr(results, \"matches\", []) or []\n",
    "\n",
    "    # 3) Show top-10 with score + preview\n",
    "    print(\"📌 Retrieved Sources (top 10):\")\n",
    "    sources_list = []\n",
    "    for i, m in enumerate(matches[:10], start=1):\n",
    "        score = m.get(\"score\", 0.0)\n",
    "        meta = m.get(\"metadata\", {}) or {}\n",
    "        filename = meta.get(\"filename\", meta.get(\"source\", f\"chunk_{i}\"))\n",
    "        text = meta.get(\"text\", meta.get(\"content\", \"\"))\n",
    "        words = text.split()\n",
    "        preview = \" \".join(words[:25]) + (\"...\" if len(words) > 25 else \"\")\n",
    "        print(f\"\\n[{i}] file={filename}  vec_score={float(score):.4f}\")\n",
    "        print(f\"Preview: {preview}\")\n",
    "        sources_list.append({\"index\": i, \"score\": float(score), \"filename\": filename, \"text\": text})\n",
    "\n",
    "    if not sources_list:\n",
    "        return \"Sorry, I couldn't find any documents to answer that question.\"\n",
    "\n",
    "    # 4) Build sources for LLM\n",
    "    sources_text = \"\\n\\n\".join(\n",
    "        [f\"[{s['index']}] (score={s['score']:.4f}) {s['filename']}\\n{s['text']}\" for s in sources_list]\n",
    "    )\n",
    "\n",
    "    # 5) Strict prompt\n",
    "    judge_prompt = f\"\"\"\n",
    "You are a strict Retrieval-Augmented Generation (RAG) assistant.\n",
    "\n",
    "RULES:\n",
    "1. Only answer using the SOURCES below.\n",
    "2. If the QUESTION cannot be answered from these sources, reply with exactly:\n",
    "   This question is out of scope of the provided documents.\n",
    "3. Do not use outside knowledge or web search.\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "SOURCES:\n",
    "{sources_text}\n",
    "\"\"\"\n",
    "\n",
    "    # 6) One LLM call\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You must refuse if the question is out of scope of provided docs.\"},\n",
    "            {\"role\": \"user\", \"content\": judge_prompt}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    resp_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # 7) Normalize refusal\n",
    "    if \"out of scope\" in resp_text.lower():\n",
    "        return \"This question is out of scope of the provided documents.\"\n",
    "\n",
    "    return resp_text\n",
    "\n",
    "# Hook into Gradio\n",
    "standalone_rag = get_answer_with_guardrails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Chatbot User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_general_knowledge(query):\n",
    "    \"\"\"Check if it's a general knowledge question (not doc-related).\"\"\"\n",
    "    keywords = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]\n",
    "    return any(word in query.lower() for word in keywords)\n",
    "\n",
    "\n",
    "def web_search(query):\n",
    "    \"\"\"Placeholder for web search logic (tooling feature goes here).\"\"\"\n",
    "    return f\"[Web Search Result Placeholder for: '{query}']\"\n",
    "\n",
    "def format_sources(results):\n",
    "    \"\"\"Format source metadata into a readable string box.\"\"\"\n",
    "    source_box = \"\"\n",
    "    for match in results['matches']:\n",
    "        doc = match['metadata'].get('source', 'Unknown Doc')\n",
    "        page = match['metadata'].get('page', 'N/A')\n",
    "        source_box += f\"📄 **{doc}** – Page {page}\\n\"\n",
    "    return source_box\n",
    "\n",
    "\n",
    "def agentic_chat_with_guardrails(query, history):\n",
    "    \"\"\"Main chat function with decision-making guardrails.\"\"\"\n",
    "\n",
    "    # Step 1: Embed the query\n",
    "    query_embedding = embed.embed_query(query)\n",
    "\n",
    "    # Step 2: Run similarity search on uploaded docs\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=10,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Agentic logic - choose RAG, Web, or Reject\n",
    "    top_score = results['matches'][0]['score'] if results['matches'] else 0\n",
    "\n",
    "    if top_score > 0.75:\n",
    "        # RAG path (use docs)\n",
    "        context = \"\\n\\n\".join([m[\"metadata\"][\"text\"] for m in results[\"matches\"]])\n",
    "        source_info = format_sources(results)\n",
    "\n",
    "        prompt = f\"\"\"Use the following context to answer the question.\n",
    "        If it cannot be answered, say \"I don't know.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer, source_info\n",
    "\n",
    "    elif is_general_knowledge(query):\n",
    "        # Web search path\n",
    "        result = web_search(query)\n",
    "        return result, \"🌐 Web Search Results\"\n",
    "\n",
    "    else:\n",
    "        # Reject unrelated queries\n",
    "        return \"❌ Sorry, I can only answer questions related to the uploaded documents or general knowledge.\", \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Keep your existing response quality function (unchanged)\n",
    "def evaluate_response_quality(response, source_chunks):\n",
    "    if not source_chunks or not response:\n",
    "        return 0.0\n",
    "    response_embed = embedding_model.encode([response])\n",
    "    source_embeds = embedding_model.encode(source_chunks)\n",
    "    scores = cosine_similarity(response_embed, source_embeds)\n",
    "    print(\"scores\", scores)\n",
    "    return float(scores.max())\n",
    "\n",
    "# Updated strict system prompt\n",
    "strict_system_prompt = \"\"\"\n",
    "You are a STRICT document-based assistant. You can ONLY answer questions that are directly related to the provided document context.\n",
    "\n",
    "STRICT RULES:\n",
    "1. If the question cannot be answered using the provided document context, respond with exactly: \"This question is outside the scope of the provided documents.\"\n",
    "2. Do not use any general knowledge or external information\n",
    "3. Do not answer questions about general topics (like food, weather, celebrities, etc.) even if you have the knowledge\n",
    "4. Only answer if the information is explicitly present in the documents\n",
    "5. The documents appear to be about Data Science, AI, and technical training - only answer questions related to these topics if they can be found in the context\n",
    "\n",
    "Examples of what to REFUSE:\n",
    "- \"What is a burger?\" → \"This question is outside the scope of the provided documents.\"\n",
    "- \"Who is the president?\" → \"This question is outside the scope of the provided documents.\"\n",
    "- \"How to cook pasta?\" → \"This question is outside the scope of the provided documents.\"\n",
    "\n",
    "Examples of what to ANSWER (only if found in documents):\n",
    "- Questions about the training program mentioned in documents\n",
    "- Questions about specific content covered in the curriculum\n",
    "- Questions about instructors or course details mentioned in the documents\n",
    "\"\"\".strip()\n",
    "\n",
    "def standalone_rag(query):\n",
    "    # 1) Embed query and search Pinecone (your existing vector DB) - NOW 10 SOURCES\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=10, include_metadata=True)\n",
    "\n",
    "    # 2) Build RAG doc context + doc source list - NOW WITH INDIVIDUAL SCORES\n",
    "    context_chunks = []\n",
    "    doc_sources = []\n",
    "    for i, match in enumerate(results.get(\"matches\", []), 1):\n",
    "        meta = match.get(\"metadata\", {}) or {}\n",
    "        text = meta.get(\"text\", \"\")\n",
    "        doc = meta.get(\"filename\", \"Unknown Doc\")\n",
    "        page = meta.get(\"page_number\", \"N/A\")\n",
    "        score = match.get(\"score\", 0.0)\n",
    "        \n",
    "        context_chunks.append(text)\n",
    "        doc_sources.append(f\"[D{i}] {doc} - Page {page} (Score: {score:.4f})\")\n",
    "\n",
    "    rag_context = \"\\n\\n\".join(context_chunks) if context_chunks else \"No relevant document context found.\"\n",
    "\n",
    "    # 3) Check relevance threshold - if top score is too low, likely off-topic\n",
    "    top_score = results.get(\"matches\", [{}])[0].get(\"score\", 0.0) if results.get(\"matches\") else 0.0\n",
    "    \n",
    "    if top_score < 0.3:  # Adjust this threshold as needed\n",
    "        return \"This question is outside the scope of the provided documents.\", \"No relevant sources found.\", \"0.000\"\n",
    "\n",
    "    # 4) LLM decision: do we need internet? (you already had this — preserving and using it)\n",
    "    decision_prompt = f\"\"\"\n",
    "You are an agent deciding if a web search is needed.\n",
    "Query: {query}\n",
    "Document Context (first 500 chars):\n",
    "{rag_context[:500]}\n",
    "\n",
    "The documents are about Data Science, AI, and technical training.\n",
    "\n",
    "Reply exactly:\n",
    "SEARCH      -> if the document context is insufficient but the question IS related to the document topics\n",
    "NO SEARCH   -> if the document context fully answers the query\n",
    "REFUSE      -> if the question is completely unrelated to the document topics (like asking about food, celebrities, general knowledge, etc.)\n",
    "\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        decision_response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": decision_prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=20\n",
    "        )\n",
    "        decision = decision_response.choices[0].message.content.strip().upper()\n",
    "    except Exception as e:\n",
    "        print(\"Decision call failed; defaulting to REFUSE for safety. Error:\", e)\n",
    "        decision = \"REFUSE\"\n",
    "\n",
    "    # 5) If REFUSE, return early\n",
    "    if \"REFUSE\" in decision:\n",
    "        return \"This question is outside the scope of the provided documents.\", \"Question deemed off-topic.\", \"0.000\"\n",
    "\n",
    "    # 6) If SEARCH -> fetch ≥10 web results, score, and prepare combined context\n",
    "    web_used = (decision == \"SEARCH\")\n",
    "    web_results_scored = []\n",
    "    web_context = \"\"\n",
    "    web_sources_display = \"\"\n",
    "\n",
    "    if web_used:\n",
    "        # at least 10 sources as required\n",
    "        raw_web = google_search_structured(query, num_results=10)\n",
    "        web_results_scored = score_sources_embeddings(query, raw_web)\n",
    "        web_sources_display = render_sources_with_scores(web_results_scored, header=\"Web Sources\")\n",
    "\n",
    "        # Build a compact context block from top web snippets (limit tokens by truncating)\n",
    "        top_snippets = [r.get(\"snippet\", \"\") for r in web_results_scored]\n",
    "        web_context = \"\\n\\n\".join(top_snippets)\n",
    "\n",
    "    # 7) Combine contexts (Docs + Optional Web)\n",
    "    combined_context = rag_context\n",
    "    if web_used and web_context:\n",
    "        combined_context += \"\\n\\n---\\n[Web Context]\\n\" + web_context\n",
    "\n",
    "    # 8) Final prompt to LLM with STRICT system prompt\n",
    "    final_prompt = f\"\"\"\n",
    "{strict_system_prompt}\n",
    "\n",
    "Context (Docs + Optional Web):\n",
    "{combined_context}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "IMPORTANT: Before answering, determine if this query can be answered from the provided context. If not, respond with exactly: \"This question is outside the scope of the provided documents.\"\n",
    "\n",
    "Rules for valid answers:\n",
    "- If you used web sources, cite them by index like [W1], [W2] that correspond to the numbered list in \"Web Sources\".\n",
    "- If you used doc sources, cite them like [D1], [D2] that correspond to the order they appear in the doc source list.\n",
    "- Be concise, accurate, and do not hallucinate.\n",
    "\"\"\".strip()\n",
    "\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        temperature=0.1,  # Lower temperature for more consistent refusals\n",
    "        max_tokens=500\n",
    "    )\n",
    "    answer = final_response.choices[0].message.content\n",
    "\n",
    "    # 9) Double-check for off-topic responses - if the LLM still answered inappropriately\n",
    "    if \"This question is outside the scope\" not in answer and top_score < 0.5:\n",
    "        # Additional safety check\n",
    "        safety_check_prompt = f\"\"\"\n",
    "Query: {query}\n",
    "Response: {answer}\n",
    "Documents are about: Data Science, AI, technical training\n",
    "\n",
    "Is this response appropriate for documents about Data Science/AI training?\n",
    "Reply: YES or NO\n",
    "\"\"\".strip()\n",
    "        \n",
    "        try:\n",
    "            safety_response = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": safety_check_prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=5\n",
    "            )\n",
    "            is_appropriate = safety_response.choices[0].message.content.strip().upper()\n",
    "            \n",
    "            if \"NO\" in is_appropriate:\n",
    "                return \"This question is outside the scope of the provided documents.\", \"Safety check triggered.\", \"0.000\"\n",
    "        except Exception as e:\n",
    "            print(\"Safety check failed:\", e)\n",
    "\n",
    "    # 10) Build the Sources panel output: WITH INDIVIDUAL SCORES\n",
    "    sources_sections = []\n",
    "    if doc_sources:\n",
    "        sources_sections.append(\"=== Document Sources (10) ===\\n\" + \"\\n\".join(doc_sources))\n",
    "    if web_used:\n",
    "        sources_sections.append(web_sources_display)\n",
    "    sources_text = \"\\n\\n\".join(sources_sections) if sources_sections else \"No sources.\"\n",
    "\n",
    "    # 11) Compute response quality against all available chunks (docs + top web snippets)\n",
    "    source_chunks_for_quality = []\n",
    "    source_chunks_for_quality.extend(context_chunks)\n",
    "    if web_used and web_results_scored:\n",
    "        source_chunks_for_quality.extend([r.get(\"snippet\", \"\") for r in web_results_scored])\n",
    "    quality = evaluate_response_quality(answer, source_chunks_for_quality)\n",
    "    quality_str = f\"{quality:.3f}\"\n",
    "\n",
    "    return answer, sources_text, quality_str\n",
    "\n",
    "# === Gradio UI (preserving your layout) ===\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RAG System\")\n",
    "    gr.Markdown(\"This assistant will only answer questions related to the provided documents (Data Science & AI training materials).\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            answer_output = gr.Textbox(label=\"Assistant\", lines=15)\n",
    "        with gr.Column(scale=1):\n",
    "            source_output = gr.Textbox(label=\"Sources\", lines=10.5)\n",
    "            quality_score = gr.Textbox(label=\"Response Quality Score\", lines=1)\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Ask a Question\", lines=1, show_label=True)\n",
    "        submit = gr.Button(\"Submit\")\n",
    "\n",
    "    def on_submit(query):\n",
    "        answer, sources, score = standalone_rag(query)\n",
    "        return answer, sources, score, \"\"\n",
    "\n",
    "    submit.click(fn=on_submit, inputs=user_input, outputs=[answer_output, source_output, quality_score, user_input])\n",
    "    user_input.submit(fn=on_submit, inputs=user_input, outputs=[answer_output, source_output, quality_score, user_input])\n",
    "\n",
    "demo.launch()\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
