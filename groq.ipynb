{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taj\\Documents\\Procurement_chatbot\\ollama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4 \n",
    "load_dotenv()\n",
    "\n",
    "# Load local embedding model (768-dim)\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_safe_and_relevant(query):\n",
    "    # Safety check\n",
    "    inappropriate_keywords = [\n",
    "        \"kill\", \"sex\", \"bomb\", \"racist\", \"suicide\", \"violence\", \"attack\", \"hack\"\n",
    "    ]\n",
    "\n",
    "    lowered_query = query.lower()\n",
    "    for word in inappropriate_keywords:\n",
    "        if word in lowered_query:\n",
    "            return False, \"Your question contains inappropriate or unsafe content.\"\n",
    "\n",
    "    # üîç LLM-based domain relevance check\n",
    "    relevance_prompt = f\"\"\"\n",
    "You are an expert assistant in data science, artificial intelligence, and linear algebra. Respond clearly, accurately, and with step-by-step reasoning when necessary.\n",
    "\n",
    "- Always assume the user may be continuing a conversation and respond appropriately to follow-ups like \"elaborate\" or \"explain more.\"\n",
    "- Do not reject questions that are vague or short; instead, ask clarifying questions if needed.\n",
    "- Avoid giving generic or evasive responses. Be specific and clear, even if the answer is limited.\n",
    "- Only say a question is unrelated if it clearly has no connection to data science, AI, or linear algebra.\n",
    "- If unsure, admit it instead of guessing or hallucinating. Say \"I'm not sure\" and explain what additional info you need.\n",
    "\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": relevance_prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.content.strip().lower() != \"yes\":\n",
    "        return False, \"Your question seems unrelated to data science or linear algebra.\"\n",
    "\n",
    "    return True, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you didn't type anything. Please feel free to ask me anything or start a conversation, and I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "question = input(\"What is your Question?\")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = 'procurement-chatbot'  # make sure this is correct in your .env\n",
    "\n",
    "if not index_name:\n",
    "    raise ValueError(\"INDEX_NAME is not set in your .env file\")\n",
    "\n",
    "index = pc.Index('procurement-chatbot')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_folder(folder_path=\"data\"):\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder '{folder_path}' does not exist. Check the path.\")\n",
    "\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(folder_path, filename))\n",
    "            pages = loader.load_and_split()\n",
    "            for i, page in enumerate(pages):\n",
    "                documents.append({\n",
    "                    \"content\": page.page_content,\n",
    "                    \"metadata\": {\"filename\": filename, \"page_number\": i + 1}\n",
    "                })\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        split_texts = text_splitter.split_text(doc[\"content\"])\n",
    "        for i, chunk_content in enumerate(split_texts):\n",
    "            chunks.append({\n",
    "                \"content\": chunk_content,\n",
    "                \"metadata\": {**doc[\"metadata\"], \"chunk_id\": i}\n",
    "            })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text=\"None\"):\n",
    "    embedding = embedding_model.encode(text).tolist()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be in one of your earlier cells (e.g., Cell 3 in our previous numbering)\n",
    "from uuid import uuid4 # Ensure this is imported at the top of your notebook\n",
    "\n",
    "def get_embedding(text=\"None\"):\n",
    "    # You can use this simpler version, or your MockResponse version, either is fine for the embedding itself\n",
    "    embedding = embedding_model.encode(text).tolist()\n",
    "    return embedding\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks, batch_size=100):\n",
    "    vectors = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        content = chunk[\"content\"]\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "\n",
    "        # IMPORTANT: This line ensures the 'text' key is added to the metadata\n",
    "        metadata[\"text\"] = content\n",
    "\n",
    "        embedding = get_embedding(content) # Get embedding after 'content' is part of metadata\n",
    "        \n",
    "        # Use uuid4 for unique IDs, consistent with your original notebook's intent\n",
    "        vector_id = str(uuid4())\n",
    "\n",
    "        vectors.append((vector_id, embedding, metadata))\n",
    "\n",
    "        if len(vectors) == batch_size or i == len(chunks) - 1:\n",
    "            index.upsert(vectors=vectors)\n",
    "            print(f\"Upserted batch ending at chunk {i + 1}\")\n",
    "            vectors = []\n",
    "    print(f\"All {len(chunks)} vectors upserted to Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 238 pages\n",
      "Generated 860 chunks\n",
      "Upserted batch ending at chunk 100\n",
      "Upserted batch ending at chunk 200\n",
      "Upserted batch ending at chunk 300\n",
      "Upserted batch ending at chunk 400\n",
      "Upserted batch ending at chunk 500\n",
      "Upserted batch ending at chunk 600\n",
      "Upserted batch ending at chunk 700\n",
      "Upserted batch ending at chunk 800\n",
      "Upserted batch ending at chunk 860\n",
      "All 860 vectors upserted to Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual path\n",
    "data_path = r\"C:\\Users\\Taj\\Documents\\Procurement_chatbot\\data\"\n",
    "\n",
    "# Load, split, embed, and upsert\n",
    "documents = load_pdfs_from_folder(data_path)\n",
    "chunks = split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "print(f\"Generated {len(chunks)} chunks\")\n",
    "\n",
    "upsert_chunks_to_pinecone(index, chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 0:\n",
      "------------------------------------------------------------\n",
      "Text:\n",
      "www.innovatics.ai\n",
      "+1 (678) 209 9780\n",
      "register@innovatics.ai\n",
      "From Zero to Hero 40-Week Training Program\n",
      " LIVE ONLINE \n",
      "TRAINING PROGRAM\n",
      "BATCH 22\n",
      "FEBruary\n",
      "202522\n",
      "nd\n",
      "40 WEEKS\n",
      "The CPD Certification\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "\n",
      "Embedding (len=768):\n",
      "[-0.01627170667052269, -0.009842539206147194, -0.04329383745789528, 0.0041566104628145695, -0.019876964390277863, -0.03534669056534767, 0.07136531174182892, 0.021396152675151825, -0.04936816915869713, 0.01842551678419113] ...\n",
      "\n",
      "Chunk 1:\n",
      "------------------------------------------------------------\n",
      "Text:\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "\n",
      "Embedding (len=768):\n",
      "[0.031858790665864944, 0.11241516470909119, -0.003929223399609327, 0.05698351189494133, -0.03818603232502937, -0.024249201640486717, 0.043073441833257675, 0.06952269375324249, -0.03651592880487442, 0.013944391161203384] ...\n",
      "\n",
      "Chunk 2:\n",
      "------------------------------------------------------------\n",
      "Text:\n",
      "/gid00035/gid00084/gid00082/gid00072/gid00077/gid00068/gid00082/gid00082/gid00001/gid00034/gid00077/gid00064/gid00075/gid00088/gid00083/gid00072/gid00066/gid00082/gid00001/gid00036/gid00068/gid00081/gid00083/gid00072/gid00195/gid00066/gid00064/gid00083/gid00072/gid00078/gid00077\n",
      "Program with 3 International Accreditations\n",
      "Certificate in \n",
      "DATA SCIENCE & AI\n",
      "\n",
      "Embedding (len=768):\n",
      "[-0.0020678075961768627, 0.040739063173532486, -0.029013393446803093, 0.00992175005376339, 0.011617114767432213, 0.026344727724790573, 0.0356917567551136, 0.0391991063952446, -0.02898559160530567, -0.010828346945345402] ...\n"
     ]
    }
   ],
   "source": [
    "# Show first 3 chunks with embeddings\n",
    "for i in range(min(3, len(chunks))):\n",
    "    content = chunks[i][\"content\"]\n",
    "    embedding = get_embedding(content)\n",
    "    \n",
    "    print(f\"\\nChunk {i}:\\n{'-'*60}\")\n",
    "    print(f\"Text:\\n{content[:500]}{'...' if len(content) > 500 else ''}\")  # Truncated view\n",
    "    print(f\"\\nEmbedding (len={len(embedding)}):\\n{embedding[:10]} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LLM Generated Response ---\n",
      "It seems like you forgot to include the question. Please go ahead and provide the question, and I'll do my best to answer it based on the provided context. If the answer is not available in the context, I will let you know that as well.\n",
      "\n",
      "--- Retrieved Relevant Chunks with Metadata ---\n",
      "\n",
      "Chunk 1:\n",
      "  Score: 0.2450\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "  Score: 0.2450\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "  Score: 0.2450\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 4:\n",
      "  Score: 0.2450\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 5:\n",
      "  Score: 0.2450\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who‚Äôs Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your embedding model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Get the user query\n",
    "user_query = input(\"Ask something: \")\n",
    "\n",
    "# Convert the query to an embedding\n",
    "query_embedding = embedding_model.encode(user_query).tolist()\n",
    "\n",
    "# Search Pinecone index\n",
    "top_k = 5\n",
    "results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "# Extract relevant chunks for LLM context\n",
    "relevant_chunks_text = [match['metadata']['text'] for match in results['matches']]\n",
    "\n",
    "# Combine chunks into a single context string\n",
    "context = \"\\n\\n\".join(relevant_chunks_text)\n",
    "\n",
    "# Formulate the prompt for the LLM\n",
    "prompt_for_llm = f\"\"\"Based on the following context, please answer the question.\n",
    "If the answer is not available in the context, state that you cannot answer from the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Call the Groq LLM for inference\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_for_llm,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\", # Or your preferred Groq model\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# Print the LLM's generated response\n",
    "print(\"\\n--- LLM Generated Response ---\")\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n",
    "# Optional: Print retrieved chunks with metadata for verification\n",
    "print(\"\\n--- Retrieved Relevant Chunks with Metadata ---\")\n",
    "for i, match in enumerate(results['matches']):\n",
    "    chunk_text = match['metadata']['text']\n",
    "    filename = match['metadata'].get('filename', 'N/A')\n",
    "    page_number = match['metadata'].get('page_number', 'N/A')\n",
    "    score = match['score']\n",
    "\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Score: {score:.4f}\")\n",
    "    print(f\"  Source File: {filename}\")\n",
    "    print(f\"  Page Number: {page_number}\")\n",
    "    print(f\"  Content:\\n{chunk_text}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_with_guardrails(query):\n",
    "    # Step 1: Guardrail check\n",
    "    is_ok, msg = is_safe_and_relevant(query)\n",
    "    if not is_ok:\n",
    "        return msg\n",
    "\n",
    "    # Step 2: Embed the query\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "\n",
    "    # Step 3: Query Pinecone for context\n",
    "    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
    "\n",
    "    # Step 4: Relevance guardrail\n",
    "    if not results.matches or results.matches[0].score < 0.75:\n",
    "        return \"Sorry, I couldn't find enough relevant information to answer that.\"\n",
    "\n",
    "    # Step 5: Compile the context\n",
    "    context = \"\\n\\n\".join([match.metadata['text'] for match in results.matches])\n",
    "\n",
    "    # Step 6: Create separate system + user prompts\n",
    "    system_prompt = \"\"\"\n",
    "You are an assistant that only answers using the provided document context.\n",
    "If the answer is not in the context, respond:\n",
    "\"I'm sorry, I can't find that information in the provided documents.\"\n",
    "\n",
    "Do not answer inappropriate or off-topic questions.\n",
    "\"\"\".strip()\n",
    "\n",
    "    user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "    # Step 7: Generate answer with llama3\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.strip()}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taj\\AppData\\Local\\Temp\\ipykernel_17052\\3244298992.py:120: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat History\", bubble_full_width=False)\n",
      "C:\\Users\\Taj\\AppData\\Local\\Temp\\ipykernel_17052\\3244298992.py:120: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(label=\"Chat History\", bubble_full_width=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Main Chat Logic with Integrated Guardrails ---\n",
    "import gradio as gr\n",
    "def rag_chat_with_guardrails(user_message, chat_history):\n",
    "    \"\"\"\n",
    "    This function combines the RAG logic with the guardrails.\n",
    "    It will be used by the Gradio interface.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. --- Input Guardrail ---\n",
    "        # Check if the query is safe and relevant before doing anything else.\n",
    "        is_ok, guardrail_msg = is_safe_and_relevant(user_message)\n",
    "        if not is_ok:\n",
    "            chat_history.append([user_message, guardrail_msg])\n",
    "            # Return an empty string for the message box, the updated history,\n",
    "            # and a message for the chunks display.\n",
    "            return \"\", chat_history, f\"Query rejected by guardrail: {guardrail_msg}\"\n",
    "\n",
    "        # 2. --- Retrieval ---\n",
    "        # If the input is valid, proceed with embedding and retrieval.\n",
    "        query_embedding = embedding_model.encode(user_message).tolist()\n",
    "        results = index.query(vector=query_embedding, top_k=5, include_metadata=True)\n",
    "\n",
    "        # ---- TEMPORARY DEBUG CODE START ----\n",
    "        # This block will print the scores so you can see them in your terminal/output.\n",
    "        if results.matches:\n",
    "            print(\"--- DEBUG: Top Match Scores ---\")\n",
    "            for match in results.matches:\n",
    "                print(f\"  Score: {match['score']:.4f}\")\n",
    "            print(\"-----------------------------\")\n",
    "        # ---- TEMPORARY DEBUG CODE END ----\n",
    "\n",
    "\n",
    "        # 3. --- Relevance Guardrail ---\n",
    "        # Adjust the threshold (e.g., to 0.5) based on the debug output you see.\n",
    "        # I've set it to 0.5 as a reasonable starting point.\n",
    "        relevance_threshold = 0.5\n",
    "        if not results.matches or results.matches[0].score < relevance_threshold:\n",
    "            response = \"Sorry, I couldn't find enough relevant information to answer that.\"\n",
    "            chat_history.append([user_message, response])\n",
    "            # This message below helps you see why it stopped in the UI.\n",
    "            top_score_msg = f\"\\n\\nTop score was: {results.matches[0].score:.4f}\" if results.matches else \"\"\n",
    "            chunks_display_str = f\"No relevant chunks found with a high enough score (>{relevance_threshold}).{top_score_msg}\"\n",
    "            return \"\", chat_history, chunks_display_str\n",
    "\n",
    "        # If chunks are relevant, prepare them for display and for the LLM.\n",
    "        relevant_chunks_text = [match['metadata']['text'] for match in results['matches']]\n",
    "        context = \"\\n\\n\".join(relevant_chunks_text)\n",
    "\n",
    "        chunks_display_str = \"\"\n",
    "        for i, match in enumerate(results['matches']):\n",
    "            chunks_display_str += (\n",
    "                f\"Chunk {i+1}: (Score: {match['score']:.4f})\\n\"\n",
    "                f\"Source: {match['metadata'].get('filename', 'N/A')}, Page: {match['metadata'].get('page_number', 'N/A')}\\n\"\n",
    "                f\"Content: {match['metadata']['text']}\\n\"\n",
    "                \"------------------------------------------------------------\\n\\n\"\n",
    "            )\n",
    "\n",
    "        # 4. --- Output Guardrail (System Prompt) ---\n",
    "        # Build the prompt with strict instructions for the LLM.\n",
    "        system_prompt = \"\"\"\n",
    "You are an assistant that only answers using the provided document context.\n",
    "If the answer is not in the context, respond:\n",
    "\"I'm sorry, I can't find that information in the provided documents.\"\n",
    "You are an expert assistant in data science, artificial intelligence, and linear algebra.\n",
    "\n",
    "- Answer with clarity, depth, and step-by-step reasoning when helpful.\n",
    "- Maintain context of the ongoing conversation to answer follow-up questions like \"explain more\" or \"elaborate.\"\n",
    "- Only answer questions relevant to data science, AI, or linear algebra.\n",
    "- If a question is unclear or lacks context, ask for clarification rather than guessing.\n",
    "- If you don‚Äôt know the answer or can't find enough information, say \"I‚Äôm not sure\" and suggest what the user can provide to help.\n",
    "Do not answer inappropriate or off-topic questions.\n",
    "\"\"\".strip()\n",
    "\n",
    "        user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {user_message}\"\n",
    "\n",
    "        # 5. --- LLM Call ---\n",
    "        # Call the LLM with the guarded prompt.\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.4,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        chat_history.append([user_message, answer])\n",
    "\n",
    "        return \"\", chat_history, chunks_display_str\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred: {e}\"\n",
    "        chat_history.append([user_message, error_message])\n",
    "        return \"\", chat_history, f\"Error during processing: {e}\"\n",
    "\n",
    "# --- Helper functions for UI interaction ---\n",
    "\n",
    "def edit_last_message(chat_history):\n",
    "    if chat_history:\n",
    "        last_user_message = chat_history[-1][0]\n",
    "        chat_history = chat_history[:-1]\n",
    "        return last_user_message, chat_history\n",
    "    return \"\", chat_history\n",
    "\n",
    "def delete_last_message(chat_history):\n",
    "    if chat_history:\n",
    "        return chat_history[:-1]\n",
    "    return chat_history\n",
    "\n",
    "# --- Gradio UI Block ---\n",
    "\n",
    "with gr.Blocks(theme=\"default\") as demo:\n",
    "    gr.Markdown(\"# ü§ñ RAG Chatbot with Guardrails\")\n",
    "    gr.Markdown(\"This chatbot answers questions based on the provided documents. It will reject inappropriate or off-topic questions.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(label=\"Chat History\", bubble_full_width=False)\n",
    "            msg = gr.Textbox(placeholder=\"Ask me about data science or linear algebra...\", label=\"Your Question\")\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Send\")\n",
    "                edit_btn = gr.Button(\"‚úèÔ∏è Edit Last\")\n",
    "                delete_btn = gr.Button(\"üóëÔ∏è Delete Last\")\n",
    "                clear_btn = gr.Button(\"‚ú® Clear Chat\")\n",
    "        with gr.Column(scale=1):\n",
    "            relevant_chunks_display = gr.Textbox(\n",
    "                label=\"Retrieved Context from Documents (RAG)\",\n",
    "                interactive=False,\n",
    "                lines=25,\n",
    "                max_lines=25,\n",
    "            )\n",
    "\n",
    "    # Wire up the UI components to the functions\n",
    "    send_btn.click(\n",
    "        rag_chat_with_guardrails,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot, relevant_chunks_display]\n",
    "    )\n",
    "    msg.submit(\n",
    "        rag_chat_with_guardrails,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot, relevant_chunks_display]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(lambda: ([], \"\", \"\"), outputs=[chatbot, msg, relevant_chunks_display])\n",
    "    edit_btn.click(edit_last_message, inputs=[chatbot], outputs=[msg, chatbot])\n",
    "    delete_btn.click(delete_last_message, inputs=[chatbot], outputs=[chatbot])\n",
    "\n",
    "# Launch the full-featured demo\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
