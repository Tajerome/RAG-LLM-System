{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries and Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from uuid import uuid4 \n",
    "from groq import Groq\n",
    "import gradio as gr\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4 \n",
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "load_dotenv()\n",
    "\n",
    "# Load local embedding model (768-dim)\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Web Search + Per-Source Scoring + Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Web Search + Per-Source Scoring + Rendering\n",
    "# This leaves your original `google_search` untouched.\n",
    "# Dependencies: GOOGLE_API_KEY / GOOGLE_SEARCH_ENGINE_ID env, `requests` imported earlier, `embedding_model` available.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def google_search_structured(query: str, num_results: int = 10):\n",
    "    \"\"\"\n",
    "    Google CSE — returns a list of dicts with title, link, snippet.\n",
    "    Prints 'Searching the internet...' (as required).\n",
    "    \"\"\"\n",
    "    print(\"Searching the internet...\")\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        \"cx\": os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\"),\n",
    "        \"q\": query,\n",
    "        \"num\": num_results,\n",
    "    }\n",
    "    resp = requests.get(url, params=params)\n",
    "    data = resp.json()\n",
    "    results = []\n",
    "    for item in data.get(\"items\", []):\n",
    "        results.append({\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"link\": item.get(\"link\", \"\"),\n",
    "            \"snippet\": item.get(\"snippet\", \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def render_sources_with_scores(results: list, header: str = \"Web Sources\"):\n",
    "    \"\"\"\n",
    "    Pretty-prints sources with individual scores (Title, URL, Snippet, Score).\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return f\"=== {header} ===\\nNo web sources found.\"\n",
    "    lines = [f\"=== {header} (Top {len(results)}) ===\"]\n",
    "    for i, r in enumerate(results, 1):\n",
    "        lines.append(\n",
    "            f\"[{i}] {r.get('title','')}\\n\"\n",
    "            f\"URL: {r.get('link','')}\\n\"\n",
    "            f\"Score: {r.get('score','0')}\\n\"\n",
    "            f\"Snippet: {r.get('snippet','')}\\n\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pinecone Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client (connecting to existing index)\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index('procurement-chatbot')\n",
    "\n",
    "# Load embedding model for queries only  \n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LLM Generated Response ---\n",
      "You haven't asked a question yet. Please go ahead and ask your question based on the provided context, and I'll do my best to answer it. If the answer is not available in the context, I'll let you know that I cannot answer from the provided information.\n",
      "\n",
      "--- Retrieved Relevant Chunks with Metadata ---\n",
      "\n",
      "Chunk 1:\n",
      "  Score: 0.2451\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "Service\n",
      "Marquis Who’s Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "/gid00042/gid00077/gid00083/gid00068/gid00081/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00078/gid00069/gid00001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "  Score: 0.1941\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 23.0\n",
      "  Content:\n",
      "Sequence Learning & GANs Week 19\n",
      "Learning Obje ctives: This week, we explore another fascinating application of neural networks: equipping \n",
      "computers to understand human language. You will learn to work with text data and sequential data, delving into the \n",
      "world of Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs).\n",
      "RNNs are designed to recognize patterns in sequences of data, making them ideal for tasks involving time series,\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "  Score: 0.1898\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 25.0\n",
      "  Content:\n",
      "WWW.iNNOVATiCS.AiDATA SCIENCE & AI ZERO TO HERO\n",
      "Advanced Natural Language Processing (NLP) Week 21\n",
      "Learning Objectives: This week, we dive into advanced Natural Language Processing (NLP) modeling. As NLP \n",
      "continues to evolve, it is transforming the way we interact with machines and manage information. Advanced NLP \n",
      "techniques, such as deep learning-based models, are pushing the boundaries of what machines can understand and\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 4:\n",
      "  Score: 0.1835\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 8.0\n",
      "  Content:\n",
      "complete overhaul of the company’s online presence, enhancing digital sales and marketing performance with a focus on data science, AI, cloud computing, and \n",
      "cybersecurity training.In addition to leading Digirabia. He is dedicated to helping businesses increase their customer base, sales, and revenue through strategic \n",
      "consulting and coaching, covering areas such as digital marketing, ad tech, mar tech, and data-driven AI marketing.\n",
      "Pioneering Leader and Innovator in Digital Transformation\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 5:\n",
      "  Score: 0.1829\n",
      "  Source File: 1- InnovatiCS - DS & AI Zero to Hero  Batch 22 ( Feb 22 -2025).pdf\n",
      "  Page Number: 1.0\n",
      "  Content:\n",
      "www.innovatics.ai\n",
      "+1 (678) 209 9780\n",
      "register@innovatics.ai\n",
      "From Zero to Hero 40-Week Training Program\n",
      " LIVE ONLINE \n",
      "TRAINING PROGRAM\n",
      "BATCH 22\n",
      "FEBruary\n",
      "202522\n",
      "nd\n",
      "40 WEEKS\n",
      "The CPD Certification\n",
      "Service\n",
      "Marquis Who’s Who 2024\n",
      "over 125 years\n",
      "Internet 2.0 Outstanding\n",
      "Leadership Award-Dubai 2022\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Get the user query\n",
    "user_query = input(\"Ask something: \")\n",
    "\n",
    "# Convert the query to an embedding\n",
    "query_embedding = embedding_model.encode(user_query).tolist()\n",
    "\n",
    "# Search Pinecone index\n",
    "top_k = 5\n",
    "results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "# Extract relevant chunks for LLM context\n",
    "relevant_chunks_text = [match['metadata']['text'] for match in results['matches']]\n",
    "\n",
    "# Combine chunks into a single context string\n",
    "context = \"\\n\\n\".join(relevant_chunks_text)\n",
    "\n",
    "# Formulate the prompt for the LLM\n",
    "prompt_for_llm = f\"\"\"Based on the following context, please answer the question.\n",
    "If the answer is not available in the context, state that you cannot answer from the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Call the Groq LLM for inference\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_for_llm,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\", # Or preferred Groq model\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# Print the LLM's generated response\n",
    "print(\"\\n--- LLM Generated Response ---\")\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n",
    "print(\"\\n--- Retrieved Relevant Chunks with Metadata ---\")\n",
    "for i, match in enumerate(results['matches']):\n",
    "    chunk_text = match['metadata']['text']\n",
    "    filename = match['metadata'].get('filename', 'N/A')\n",
    "    page_number = match['metadata'].get('page_number', 'N/A')\n",
    "    score = match['score']\n",
    "\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Score: {score:.4f}\")\n",
    "    print(f\"  Source File: {filename}\")\n",
    "    print(f\"  Page Number: {page_number}\")\n",
    "    print(f\"  Content:\\n{chunk_text}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Chatbot User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the internet...\n",
      "scores [[0.8800718  0.811766   0.71024203 0.685486   0.61453116 0.5918271\n",
      "  0.53871787 0.5460706  0.52586555 0.48185322 0.88180876 0.8283993\n",
      "  0.7997967  0.7735303  0.6209624  0.61730707 0.6263708  0.44840848\n",
      "  0.60307056 0.45156747]]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Keep your existing response quality function (unchanged)\n",
    "def evaluate_response_quality(response, source_chunks):\n",
    "    if not source_chunks or not response:\n",
    "        return 0.0\n",
    "    response_embed = embedding_model.encode([response])\n",
    "    source_embeds = embedding_model.encode(source_chunks)\n",
    "    scores = cosine_similarity(response_embed, source_embeds)\n",
    "    print(\"scores\", scores)\n",
    "    return float(scores.max())\n",
    "\n",
    "# Updated strict system prompt\n",
    "strict_system_prompt = \"\"\"\n",
    "You are a STRICT document-based assistant. You can ONLY answer questions that are directly related to the provided document context.\n",
    "\n",
    "STRICT RULES:\n",
    "1. If the question cannot be answered using the provided document context, respond with exactly: \"This question is outside the scope of the provided documents.\"\n",
    "2. Do not use any general knowledge or external information\n",
    "3. Do not answer questions about general topics (like food, weather, celebrities, etc.) even if you have the knowledge\n",
    "4. Only answer if the information is explicitly present in the documents\n",
    "5. Questions related to topics covered in the provided documents ARE in scope\n",
    "\"\"\".strip()\n",
    "\n",
    "# Helper function to score web results\n",
    "def score_web_results(query, web_results):\n",
    "    \"\"\"Score web results by cosine similarity to query.\"\"\"\n",
    "    if not web_results:\n",
    "        return []\n",
    "    \n",
    "    query_embed = embedding_model.encode([query])[0]\n",
    "    scored_results = []\n",
    "    \n",
    "    for result in web_results:\n",
    "        snippet = result.get(\"snippet\", \"\")\n",
    "        if snippet:\n",
    "            snippet_embed = embedding_model.encode([snippet])[0]\n",
    "            score = cosine_similarity([query_embed], [snippet_embed])[0][0]\n",
    "            result[\"score\"] = float(score)\n",
    "            scored_results.append(result)\n",
    "    \n",
    "    # Sort by score descending\n",
    "    scored_results.sort(key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "    return scored_results\n",
    "\n",
    "def standalone_rag(query):\n",
    "    try:\n",
    "        # 1) Embed query and search Pinecone\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        results = index.query(vector=query_embedding, top_k=10, include_metadata=True)\n",
    "\n",
    "        # 2) Build RAG doc context + doc source list\n",
    "        context_chunks = []\n",
    "        doc_sources = []\n",
    "        for i, match in enumerate(results.get(\"matches\", []), 1):\n",
    "            meta = match.get(\"metadata\", {}) or {}\n",
    "            text = meta.get(\"text\", \"\")\n",
    "            doc = meta.get(\"filename\", \"Unknown Doc\")\n",
    "            page = meta.get(\"page_number\", \"N/A\")\n",
    "            score = match.get(\"score\", 0.0)\n",
    "            \n",
    "            context_chunks.append(text)\n",
    "            doc_sources.append(f\"[D{i}] {doc} - Page {page} (Score: {score:.4f})\")\n",
    "\n",
    "        rag_context = \"\\n\\n\".join(context_chunks) if context_chunks else \"No relevant document context found.\"\n",
    "        top_score = results.get(\"matches\", [{}])[0].get(\"score\", 0.0) if results.get(\"matches\") else 0.0\n",
    "\n",
    "        # 3) LLM decision: Is query related to docs? Need web search?\n",
    "        decision_prompt = f\"\"\"\n",
    "You are an agent evaluating if a query can be answered from the provided documents.\n",
    "\n",
    "Query: {query}\n",
    "Top Vector Score: {top_score:.4f}\n",
    "Document Context (first 800 chars):\n",
    "{rag_context[:800]}\n",
    "\n",
    "Analyze:\n",
    "1. Is this query related to topics covered in the provided documents?\n",
    "2. Can it be answered from the provided context?\n",
    "\n",
    "Reply with EXACTLY ONE of these:\n",
    "- \"NO SEARCH\" → Query is related to the documents AND answerable from the context\n",
    "- \"SEARCH\" → Query is related to the documents BUT needs additional web context\n",
    "- \"REFUSE\" → Query is completely unrelated to the document topics\n",
    "\n",
    "Consider: If the query is about a topic that appears to be covered in the documents (based on the context), treat it as related even if the exact answer isn't found.\n",
    "\"\"\".strip()\n",
    "\n",
    "        decision_response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": decision_prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=20\n",
    "        )\n",
    "        decision = decision_response.choices[0].message.content.strip().upper()\n",
    "\n",
    "        # 4) If REFUSE, return early - LLM decided it's off-topic\n",
    "        if \"REFUSE\" in decision:\n",
    "            return \"This question is outside the scope of the provided documents.\", \"No relevant sources (off-topic query).\", \"0.000\"\n",
    "\n",
    "        # 5) If SEARCH -> fetch web results\n",
    "        web_used = (\"SEARCH\" in decision)\n",
    "        web_results_scored = []\n",
    "        web_context = \"\"\n",
    "        web_sources_display = \"\"\n",
    "\n",
    "        if web_used:\n",
    "            raw_web = google_search_structured(query, num_results=10)\n",
    "            web_results_scored = score_web_results(query, raw_web)\n",
    "            web_sources_display = render_sources_with_scores(web_results_scored, header=\"Web Sources\")\n",
    "\n",
    "            top_snippets = [r.get(\"snippet\", \"\") for r in web_results_scored]\n",
    "            web_context = \"\\n\\n\".join(top_snippets)\n",
    "\n",
    "        # 6) Combine contexts\n",
    "        combined_context = rag_context\n",
    "        if web_used and web_context:\n",
    "            combined_context += \"\\n\\n---\\n[Web Context]\\n\" + web_context\n",
    "\n",
    "        # 7) Final prompt - LLM answers based on combined context\n",
    "        final_prompt = f\"\"\"\n",
    "{strict_system_prompt}\n",
    "\n",
    "Context (Docs + Optional Web):\n",
    "{combined_context}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Instructions:\n",
    "1. First determine: Can this be answered from the provided context?\n",
    "2. If YES → Answer it clearly and include inline citations like [D1], [D2] for documents or [W1], [W2] for web sources naturally within your answer\n",
    "3. If NO → Respond exactly: \"This question is outside the scope of the provided documents.\"\n",
    "\n",
    "IMPORTANT: \n",
    "- Include citations WITHIN your answer text (e.g., \"According to [D1], the solution is...\")\n",
    "- DO NOT create a separate \"References:\" or \"Sources:\" section at the end\n",
    "- Just provide a natural answer with inline citations\n",
    "\"\"\".strip()\n",
    "\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        answer = final_response.choices[0].message.content\n",
    "\n",
    "        # If answer says it's out of scope, clear sources\n",
    "        if \"outside the scope\" in answer.lower():\n",
    "            return answer, \"No relevant sources (query could not be answered).\", \"0.000\"\n",
    "\n",
    "        # 8) Build sources output\n",
    "        sources_sections = []\n",
    "        if doc_sources:\n",
    "            sources_sections.append(\"=== Document Sources (10) ===\\n\" + \"\\n\".join(doc_sources))\n",
    "        if web_used:\n",
    "            sources_sections.append(web_sources_display)\n",
    "        sources_text = \"\\n\\n\".join(sources_sections) if sources_sections else \"No sources.\"\n",
    "\n",
    "        # 9) Compute response quality\n",
    "        source_chunks_for_quality = context_chunks[:]\n",
    "        if web_used and web_results_scored:\n",
    "            source_chunks_for_quality.extend([r.get(\"snippet\", \"\") for r in web_results_scored])\n",
    "        quality = evaluate_response_quality(answer, source_chunks_for_quality)\n",
    "        quality_str = f\"{quality:.3f}\"\n",
    "\n",
    "        return answer, sources_text, quality_str\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg, \"Error occurred\", \"0.000\"\n",
    "\n",
    "# === Gradio UI ===\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RAG System\")\n",
    "    gr.Markdown(\"This assistant answers questions related to the provided documents, supplemented by web search when needed.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            answer_output = gr.Textbox(label=\"Assistant\", lines=15)\n",
    "        with gr.Column(scale=1):\n",
    "            source_output = gr.Textbox(label=\"Sources\", lines=10.5)\n",
    "            quality_score = gr.Textbox(label=\"Response Quality Score\", lines=1)\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Ask a Question\", lines=1, show_label=True)\n",
    "        submit = gr.Button(\"Submit\")\n",
    "\n",
    "    def on_submit(query):\n",
    "        answer, sources, score = standalone_rag(query)\n",
    "        return answer, sources, score, \"\"\n",
    "\n",
    "    submit.click(fn=on_submit, inputs=user_input, outputs=[answer_output, source_output, quality_score, user_input])\n",
    "    user_input.submit(fn=on_submit, inputs=user_input, outputs=[answer_output, source_output, quality_score, user_input])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
